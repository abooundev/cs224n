# part1. Word Vectors I: Introduction, SVD and Word2Vec

**1 Introduction to Natural Language Processing**
1.1 What is so special about NLP?
1.2 Examples of tasks
**2 Word Vectors**
**3 SVD Based Methods**
3.1 Word-Document Matrix
3.2 Window based Co-occurrence Matrix
3.3 Applying SVD to the cooccurrence matrix
**4 Iteration Based Methods - Word2vec**
4.1 Language Models (Unigrams, Bigrams, etc)
4.2 Continuous Bag of Words Model (CBOW)
4.3 Skip-Gram Model
4.4 Negative Sampling
4.5 Hierarchical Softmax

# part2. Word Vectors II: GloVe, Evaluation and Training 
**1 Global Vectors for Word Representation (GloVe)**
1.1 Comparison with Previous Methods
1.2 Co-occurrence Matrix
1.3 Least Squares Objective
1.4 Conclusion
**2 Evaluation of Word Vectors**
2.1 Intrinsic Evaluation
2.2 Extrinsic Evaluation
2.3 Intrinsic Evaluation Example: Word Vector Analogies
2.4 Intrinsic Evaluation Tuning Example: Analogy Evaluations
2.5 Intrinsic Evaluation Example: Correlation Evaluation
2.6 Further Reading: Dealing With Ambiguity
**3 Training for Extrinsic Tasks**
3.1 Problem Formulation
3.2 Retraining Word Vectors
3.3 Softmax Classification and Regularization
3.4 Window Classification
3.5 Non-linear Classifiers

# part3. Neural Networks, Backpropagation 
**1 Neural Networks: Foundations**
1.1 A Neuron
1.2 A Single Layer of Neurons
1.3 Feed-forward Computation
1.4 Maximum Margin Objective Function
1.5 Training with Backpropagation â€“ Elemental
**2 Neural Networks: Tips and Tricks**
2.1 Gradient Check
2.2 Regularization
2.3 Dropout
2.4 Neuron Units
2.5 Data Preprocessing
2.6 Parameter Initialization
2.7 Learning Strategies
2.8 Momentum Updates
2.9 Adaptive Optimization Methods

